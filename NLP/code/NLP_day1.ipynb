{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**텍스트 전처리**   \n",
    "-토큰(글자, 단어, 문장, 문단)화, 단어 통일(is, are, were, ...)   \n",
    "-불용어   \n",
    "-정규표현식   \n",
    "\n",
    "**인코딩(단어 -> 수치)** ex) one-hot encoding   \n",
    "**패딩(글자수 맞추기)**\n",
    "\n",
    "-한글 -> konlpy.org"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**자연어처리 환경 구성**\n",
    "\n",
    "1. 텐서플로우   \n",
    "pip install tensorflow\n",
    "\n",
    "2. nltk   \n",
    "pip install nltk   \n",
    "import nltk   \n",
    "nltk.download('all')\n",
    "\n",
    "3. konlpy 설치   \n",
    "pip install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.6.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import konlpy\n",
    "konlpy.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['단독', '입찰', '보다', '복수', '입찰', '의', '경우']\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()\n",
    "print(okt.morphs(u'단독입찰보다 복수입찰의 경우'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['아버지', '가방', '에', '들어가신다']\n"
     ]
    }
   ],
   "source": [
    "print(okt.morphs(u'아버지가방에들어가신다'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, WordPunctTokenizer, sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화 결과:  ['Tommy', \"'s\", 'Do', \"n't\", 'And', 'that', '’', 's', 'exactly', 'the', 'way', 'with', 'our', 'machines', '.', 'In', 'order', 'to', 'get', 'our', 'computer', 'to', 'understand', 'any', 'text', ',', 'we', 'need', 'to', 'break', 'that', 'word', 'down', 'in', 'a', 'way', 'that', 'our', 'machine', 'can', 'understand', '.', 'That', '’', 's', 'where', 'the', 'concept', 'of', 'tokenization', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', 'in', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"단어 토큰화 결과: \", word_tokenize(\"Tommy's Don't And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "단어 토큰화 결과:  ['Tommy', \"'\", 's', 'Don', \"'\", 't', 'And', 'that', '’', 's', 'exactly', 'the', 'way', 'with', 'our', 'machines', '.', 'In', 'order', 'to', 'get', 'our', 'computer', 'to', 'understand', 'any', 'text', ',', 'we', 'need', 'to', 'break', 'that', 'word', 'down', 'in', 'a', 'way', 'that', 'our', 'machine', 'can', 'understand', '.', 'That', '’', 's', 'where', 'the', 'concept', 'of', 'tokenization', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', 'in', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"단어 토큰화 결과: \", WordPunctTokenizer().tokenize(\"Tommy's Don't And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장 토큰화 결과:  ['Language is a thing of beauty.', 'But mastering a new language from scratch is quite a daunting prospect.', 'If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this!', 'There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.']\n"
     ]
    }
   ],
   "source": [
    "data = \"Language is a thing of beauty. But mastering a new language from scratch is quite a daunting prospect. If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this! There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.\"\n",
    "print('문장 토큰화 결과: ', sent_tokenize(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Tommy', 'NNP'),\n",
       " (\"'s\", 'POS'),\n",
       " ('Do', 'VBP'),\n",
       " (\"n't\", 'RB'),\n",
       " ('And', 'CC'),\n",
       " ('that', 'IN'),\n",
       " ('’', 'NNP'),\n",
       " ('s', 'VBZ'),\n",
       " ('exactly', 'RB'),\n",
       " ('the', 'DT'),\n",
       " ('way', 'NN'),\n",
       " ('with', 'IN'),\n",
       " ('our', 'PRP$'),\n",
       " ('machines', 'NNS'),\n",
       " ('.', '.'),\n",
       " ('In', 'IN'),\n",
       " ('order', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('get', 'VB'),\n",
       " ('our', 'PRP$'),\n",
       " ('computer', 'NN'),\n",
       " ('to', 'TO'),\n",
       " ('understand', 'VB'),\n",
       " ('any', 'DT'),\n",
       " ('text', 'NN'),\n",
       " (',', ','),\n",
       " ('we', 'PRP'),\n",
       " ('need', 'VBP'),\n",
       " ('to', 'TO'),\n",
       " ('break', 'VB'),\n",
       " ('that', 'IN'),\n",
       " ('word', 'NN'),\n",
       " ('down', 'RP'),\n",
       " ('in', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('way', 'NN'),\n",
       " ('that', 'IN'),\n",
       " ('our', 'PRP$'),\n",
       " ('machine', 'NN'),\n",
       " ('can', 'MD'),\n",
       " ('understand', 'VB'),\n",
       " ('.', '.'),\n",
       " ('That', 'DT'),\n",
       " ('’', 'VBZ'),\n",
       " ('s', 'NN'),\n",
       " ('where', 'WRB'),\n",
       " ('the', 'DT'),\n",
       " ('concept', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('tokenization', 'NN'),\n",
       " ('in', 'IN'),\n",
       " ('Natural', 'NNP'),\n",
       " ('Language', 'NNP'),\n",
       " ('Processing', 'NNP'),\n",
       " ('(', '('),\n",
       " ('NLP', 'NNP'),\n",
       " (')', ')'),\n",
       " ('comes', 'VBZ'),\n",
       " ('in', 'IN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "res = word_tokenize(\"Tommy's Don't And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\")\n",
    "pos_tag(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**pip install kss**      \n",
    "\n",
    "한국어 문장 토큰화 도구   \n",
    "문서나 문단에서 문장 사이의 관계 등을 파악해야 하는 상황에서 쓰일 수 있다. 하지만 대부분은 단어 단위 토큰화를 쓴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"여름입니다. 날씨가 덥습니다! 딥러닝을 공부합니다. 네?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Kss]: Because there's no supported C++ morpheme analyzer, Kss will take pecab as a backend. :D\n",
      "For your information, Kss also supports mecab backend.\n",
      "We recommend you to install mecab or konlpy.tag.Mecab for faster execution of Kss.\n",
      "Please refer to following web sites for details:\n",
      "- mecab: https://cleancode-ws.tistory.com/97\n",
      "- konlpy.tag.Mecab: https://uwgdqo.tistory.com/363\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['여름입니다.', '날씨가 덥습니다!', '딥러닝을 공부합니다.', '네?']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kss.split_sentences(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt, Kkma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt = Okt()\n",
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okt : ['NLP', '를', '열심히', '공부', '하고', ',', '취업', '에', '성공합시다']\n",
      "Okt : [('NLP', 'Alpha'), ('를', 'Noun'), ('열심히', 'Adverb'), ('공부', 'Noun'), ('하고', 'Josa'), (',', 'Punctuation'), ('취업', 'Noun'), ('에', 'Josa'), ('성공합시다', 'Adjective')]\n",
      "Okt : ['를', '공부', '취업']\n"
     ]
    }
   ],
   "source": [
    "print(\"Okt :\", okt.morphs('NLP를 열심히 공부하고, 취업에 성공합시다'))\n",
    "print(\"Okt :\", okt.pos('NLP를 열심히 공부하고, 취업에 성공합시다'))\n",
    "print(\"Okt :\", okt.nouns('NLP를 열심히 공부하고, 취업에 성공합시다'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kkma : ['NLP', '를', '열심히', '공부', '하', '고', ',', '취업', '에', '성공', '합', '시다']\n",
      "Kkma : [('NLP', 'OL'), ('를', 'JKO'), ('열심히', 'MAG'), ('공부', 'NNG'), ('하', 'XSV'), ('고', 'ECE'), (',', 'SP'), ('취업', 'NNG'), ('에', 'JKM'), ('성공', 'NNG'), ('합', 'NNG'), ('시다', 'NNG')]\n",
      "Kkma : ['공부', '취업', '성공', '성공합시다', '합', '시다']\n"
     ]
    }
   ],
   "source": [
    "print(\"Kkma :\", kkma.morphs('NLP를 열심히 공부하고, 취업에 성공합시다'))\n",
    "print(\"Kkma :\", kkma.pos('NLP를 열심히 공부하고, 취업에 성공합시다'))\n",
    "print(\"Kkma :\", kkma.nouns('NLP를 열심히 공부하고, 취업에 성공합시다'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "빈도수가 낮거나, 길이가 매우 짧은 단어는 상황에 따라 제거 고려"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re # 정규표현식\n",
    "\n",
    "text = \"Tommy's Don't And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat = re.compile(r'\\W*\\b\\w{1,2}\\b') # 단어가 1글자 이상, 2글자 이하인거 삭제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tommy Don And that exactly the way with our machines order get our computer understand any text need break that word down way that our machine can understand. That where the concept tokenization Natural Language Processing (NLP) comes.'"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pat.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tommy's Don't And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = set(stopwords.words('english'))\n",
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = word_tokenize(text)\n",
    "res = []\n",
    "\n",
    "for w in wt:\n",
    "    if w not in sw:\n",
    "        res.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tommy', \"'s\", 'Do', \"n't\", 'And', 'that', '’', 's', 'exactly', 'the', 'way', 'with', 'our', 'machines', '.', 'In', 'order', 'to', 'get', 'our', 'computer', 'to', 'understand', 'any', 'text', ',', 'we', 'need', 'to', 'break', 'that', 'word', 'down', 'in', 'a', 'way', 'that', 'our', 'machine', 'can', 'understand', '.', 'That', '’', 's', 'where', 'the', 'concept', 'of', 'tokenization', 'in', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', 'in', '.']\n"
     ]
    }
   ],
   "source": [
    "print(wt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60\n"
     ]
    }
   ],
   "source": [
    "print(len(wt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tommy', \"'s\", 'Do', \"n't\", 'And', '’', 'exactly', 'way', 'machines', '.', 'In', 'order', 'get', 'computer', 'understand', 'text', ',', 'need', 'break', 'word', 'way', 'machine', 'understand', '.', 'That', '’', 'concept', 'tokenization', 'Natural', 'Language', 'Processing', '(', 'NLP', ')', 'comes', '.']\n",
      "36\n"
     ]
    }
   ],
   "source": [
    "print(res)\n",
    "print(len(res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"NLP를 열심히 공부하고, 취업에 성공합시다\"\n",
    "sw = \"를 에 고 라고 다\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['를', '에', '고', '라고', '다']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sw = sw.split(\" \")\n",
    "sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "wt = okt.morphs(text)\n",
    "\n",
    "res = [w for w in wt if w not in sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NLP', '를', '열심히', '공부', '하고', ',', '취업', '에', '성공합시다']\n",
      "['NLP', '열심히', '공부', '하고', ',', '취업', '성공합시다']\n"
     ]
    }
   ],
   "source": [
    "print(wt)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "원핫인코딩   \n",
    "단어 -> 정수화(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Tokenization is a key (and mandatory) aspect of working with text data\n",
    "We’ll discuss the various nuances of tokenization, including how to handle Out-of-Vocabulary words (OOV)\n",
    "Language is a thing of beauty. But mastering a new language from scratch is quite a daunting prospect. If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this! There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.\n",
    "And that’s exactly the way with our machines. In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand. That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.\n",
    "Simply put, we can’t work with text data if we don’t perform tokenization. Yes, it’s really that important!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nTokenization is a key (and mandatory) aspect of working with text data\\nWe’ll discuss the various nuances of tokenization, including how to handle Out-of-Vocabulary words (OOV)\\nLanguage is a thing of beauty.',\n",
       " 'But mastering a new language from scratch is quite a daunting prospect.',\n",
       " 'If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this!',\n",
       " 'There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.',\n",
       " 'And that’s exactly the way with our machines.',\n",
       " 'In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand.',\n",
       " 'That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.',\n",
       " 'Simply put, we can’t work with text data if we don’t perform tokenization.',\n",
       " 'Yes, it’s really that important!']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = sent_tokenize(text)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tokenization', 'key', 'and', 'mandatory', 'aspect', 'working', 'with', 'text', 'data', 'discuss', 'the', 'various', 'nuances', 'tokenization', 'including', 'how', 'handle', 'out-of-vocabulary', 'words', 'oov', 'language', 'thing', 'beauty'], ['but', 'mastering', 'new', 'language', 'from', 'scratch', 'quite', 'daunting', 'prospect'], ['you', 'ever', 'picked', 'language', 'that', 'wasn', 'your', 'mother', 'tongue', 'you', 'relate', 'this'], ['there', 'are', 'many', 'layers', 'peel', 'off', 'and', 'syntaxes', 'consider', 'quite', 'challenge'], ['and', 'that', 'exactly', 'the', 'way', 'with', 'our', 'machines'], ['order', 'get', 'our', 'computer', 'understand', 'any', 'text', 'need', 'break', 'that', 'word', 'down', 'way', 'that', 'our', 'machine', 'can', 'understand'], ['that', 'where', 'the', 'concept', 'tokenization', 'natural', 'language', 'processing', 'nlp', 'comes'], ['simply', 'put', 'can', 'work', 'with', 'text', 'data', 'don', 'perform', 'tokenization'], ['yes', 'really', 'that', 'important']]\n"
     ]
    }
   ],
   "source": [
    "vocab = {}\n",
    "pre_sents = []\n",
    "\n",
    "for s in sents:\n",
    "    wt = word_tokenize(s)\n",
    "    res = []\n",
    "    for w in wt:\n",
    "        w = w.lower()\n",
    "        if w not in sw:\n",
    "            if len(w) > 2:\n",
    "                res.append(w)\n",
    "                if w not in vocab:\n",
    "                    vocab[w] = 0\n",
    "                vocab[w] += 1\n",
    "    pre_sents.append(res)\n",
    "\n",
    "print(pre_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tokenization': 4,\n",
       " 'key': 1,\n",
       " 'and': 3,\n",
       " 'mandatory': 1,\n",
       " 'aspect': 1,\n",
       " 'working': 1,\n",
       " 'with': 3,\n",
       " 'text': 3,\n",
       " 'data': 2,\n",
       " 'discuss': 1,\n",
       " 'the': 3,\n",
       " 'various': 1,\n",
       " 'nuances': 1,\n",
       " 'including': 1,\n",
       " 'how': 1,\n",
       " 'handle': 1,\n",
       " 'out-of-vocabulary': 1,\n",
       " 'words': 1,\n",
       " 'oov': 1,\n",
       " 'language': 4,\n",
       " 'thing': 1,\n",
       " 'beauty': 1,\n",
       " 'but': 1,\n",
       " 'mastering': 1,\n",
       " 'new': 1,\n",
       " 'from': 1,\n",
       " 'scratch': 1,\n",
       " 'quite': 2,\n",
       " 'daunting': 1,\n",
       " 'prospect': 1,\n",
       " 'you': 2,\n",
       " 'ever': 1,\n",
       " 'picked': 1,\n",
       " 'that': 6,\n",
       " 'wasn': 1,\n",
       " 'your': 1,\n",
       " 'mother': 1,\n",
       " 'tongue': 1,\n",
       " 'relate': 1,\n",
       " 'this': 1,\n",
       " 'there': 1,\n",
       " 'are': 1,\n",
       " 'many': 1,\n",
       " 'layers': 1,\n",
       " 'peel': 1,\n",
       " 'off': 1,\n",
       " 'syntaxes': 1,\n",
       " 'consider': 1,\n",
       " 'challenge': 1,\n",
       " 'exactly': 1,\n",
       " 'way': 2,\n",
       " 'our': 3,\n",
       " 'machines': 1,\n",
       " 'order': 1,\n",
       " 'get': 1,\n",
       " 'computer': 1,\n",
       " 'understand': 2,\n",
       " 'any': 1,\n",
       " 'need': 1,\n",
       " 'break': 1,\n",
       " 'word': 1,\n",
       " 'down': 1,\n",
       " 'machine': 1,\n",
       " 'can': 2,\n",
       " 'where': 1,\n",
       " 'concept': 1,\n",
       " 'natural': 1,\n",
       " 'processing': 1,\n",
       " 'nlp': 1,\n",
       " 'comes': 1,\n",
       " 'simply': 1,\n",
       " 'put': 1,\n",
       " 'work': 1,\n",
       " 'don': 1,\n",
       " 'perform': 1,\n",
       " 'yes': 1,\n",
       " 'really': 1,\n",
       " 'important': 1}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('that', 6),\n",
       " ('tokenization', 4),\n",
       " ('language', 4),\n",
       " ('and', 3),\n",
       " ('with', 3),\n",
       " ('text', 3),\n",
       " ('the', 3),\n",
       " ('our', 3),\n",
       " ('data', 2),\n",
       " ('quite', 2),\n",
       " ('you', 2),\n",
       " ('way', 2),\n",
       " ('understand', 2),\n",
       " ('can', 2),\n",
       " ('key', 1),\n",
       " ('mandatory', 1),\n",
       " ('aspect', 1),\n",
       " ('working', 1),\n",
       " ('discuss', 1),\n",
       " ('various', 1),\n",
       " ('nuances', 1),\n",
       " ('including', 1),\n",
       " ('how', 1),\n",
       " ('handle', 1),\n",
       " ('out-of-vocabulary', 1),\n",
       " ('words', 1),\n",
       " ('oov', 1),\n",
       " ('thing', 1),\n",
       " ('beauty', 1),\n",
       " ('but', 1),\n",
       " ('mastering', 1),\n",
       " ('new', 1),\n",
       " ('from', 1),\n",
       " ('scratch', 1),\n",
       " ('daunting', 1),\n",
       " ('prospect', 1),\n",
       " ('ever', 1),\n",
       " ('picked', 1),\n",
       " ('wasn', 1),\n",
       " ('your', 1),\n",
       " ('mother', 1),\n",
       " ('tongue', 1),\n",
       " ('relate', 1),\n",
       " ('this', 1),\n",
       " ('there', 1),\n",
       " ('are', 1),\n",
       " ('many', 1),\n",
       " ('layers', 1),\n",
       " ('peel', 1),\n",
       " ('off', 1),\n",
       " ('syntaxes', 1),\n",
       " ('consider', 1),\n",
       " ('challenge', 1),\n",
       " ('exactly', 1),\n",
       " ('machines', 1),\n",
       " ('order', 1),\n",
       " ('get', 1),\n",
       " ('computer', 1),\n",
       " ('any', 1),\n",
       " ('need', 1),\n",
       " ('break', 1),\n",
       " ('word', 1),\n",
       " ('down', 1),\n",
       " ('machine', 1),\n",
       " ('where', 1),\n",
       " ('concept', 1),\n",
       " ('natural', 1),\n",
       " ('processing', 1),\n",
       " ('nlp', 1),\n",
       " ('comes', 1),\n",
       " ('simply', 1),\n",
       " ('put', 1),\n",
       " ('work', 1),\n",
       " ('don', 1),\n",
       " ('perform', 1),\n",
       " ('yes', 1),\n",
       " ('really', 1),\n",
       " ('important', 1)]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vs = sorted(vocab.items(), key=lambda x : x[1], reverse=True)\n",
    "vs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'that': 1,\n",
       " 'tokenization': 2,\n",
       " 'language': 3,\n",
       " 'and': 4,\n",
       " 'with': 5,\n",
       " 'text': 6,\n",
       " 'the': 7,\n",
       " 'our': 8,\n",
       " 'data': 9,\n",
       " 'quite': 10,\n",
       " 'you': 11,\n",
       " 'way': 12,\n",
       " 'understand': 13,\n",
       " 'can': 14}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 빈도수가 2이상인 단어들에 대해 index부여\n",
    "word_index = {}\n",
    "a = 0\n",
    "\n",
    "for w, f in vs:\n",
    "    if f >= 2:\n",
    "        a += 1\n",
    "        word_index[w] = a\n",
    "\n",
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nTokenization is a key (and mandatory) aspect of working with text data\\nWe’ll discuss the various nuances of tokenization, including how to handle Out-of-Vocabulary words (OOV)\\nLanguage is a thing of beauty.',\n",
       " 'But mastering a new language from scratch is quite a daunting prospect.',\n",
       " 'If you’ve ever picked up a language that wasn’t your mother tongue, you’ll relate to this!',\n",
       " 'There are so many layers to peel off and syntaxes to consider – it’s quite a challenge.',\n",
       " 'And that’s exactly the way with our machines.',\n",
       " 'In order to get our computer to understand any text, we need to break that word down in a way that our machine can understand.',\n",
       " 'That’s where the concept of tokenization in Natural Language Processing (NLP) comes in.',\n",
       " 'Simply put, we can’t work with text data if we don’t perform tokenization.',\n",
       " 'Yes, it’s really that important!']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'that': 1,\n",
       " 'tokenization': 2,\n",
       " 'language': 3,\n",
       " 'and': 4,\n",
       " 'with': 5,\n",
       " 'text': 6,\n",
       " 'the': 7,\n",
       " 'our': 8,\n",
       " 'data': 9,\n",
       " 'quite': 10,\n",
       " 'you': 11,\n",
       " 'way': 12,\n",
       " 'understand': 13,\n",
       " 'can': 14}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index['OOV'] = len(word_index) + 1 # Out Of Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2,\n",
       "  15,\n",
       "  4,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  5,\n",
       "  6,\n",
       "  9,\n",
       "  15,\n",
       "  7,\n",
       "  15,\n",
       "  15,\n",
       "  2,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  15,\n",
       "  3,\n",
       "  15,\n",
       "  15],\n",
       " [15, 15, 15, 3, 15, 15, 10, 15, 15],\n",
       " [11, 15, 15, 3, 1, 15, 15, 15, 15, 11, 15, 15],\n",
       " [15, 15, 15, 15, 15, 15, 4, 15, 15, 10, 15],\n",
       " [4, 1, 15, 7, 12, 5, 8, 15],\n",
       " [15, 15, 8, 15, 13, 15, 6, 15, 15, 1, 15, 15, 12, 1, 8, 15, 14, 13],\n",
       " [1, 15, 7, 15, 2, 15, 3, 15, 15, 15],\n",
       " [15, 15, 14, 15, 5, 6, 9, 15, 15, 2],\n",
       " [15, 15, 1, 15]]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_sents = []\n",
    "\n",
    "for s in pre_sents:\n",
    "    enc_sent = []\n",
    "    for w in s:\n",
    "        try:\n",
    "            enc_sent.append(word_index[w])\n",
    "        except KeyError:\n",
    "            enc_sent.append(word_index['OOV'])\n",
    "    enc_sents.append(enc_sent)\n",
    "    \n",
    "enc_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['tokenization',\n",
       "  'key',\n",
       "  'and',\n",
       "  'mandatory',\n",
       "  'aspect',\n",
       "  'working',\n",
       "  'with',\n",
       "  'text',\n",
       "  'data',\n",
       "  'discuss',\n",
       "  'the',\n",
       "  'various',\n",
       "  'nuances',\n",
       "  'tokenization',\n",
       "  'including',\n",
       "  'how',\n",
       "  'handle',\n",
       "  'out-of-vocabulary',\n",
       "  'words',\n",
       "  'oov',\n",
       "  'language',\n",
       "  'thing',\n",
       "  'beauty'],\n",
       " ['but',\n",
       "  'mastering',\n",
       "  'new',\n",
       "  'language',\n",
       "  'from',\n",
       "  'scratch',\n",
       "  'quite',\n",
       "  'daunting',\n",
       "  'prospect'],\n",
       " ['you',\n",
       "  'ever',\n",
       "  'picked',\n",
       "  'language',\n",
       "  'that',\n",
       "  'wasn',\n",
       "  'your',\n",
       "  'mother',\n",
       "  'tongue',\n",
       "  'you',\n",
       "  'relate',\n",
       "  'this'],\n",
       " ['there',\n",
       "  'are',\n",
       "  'many',\n",
       "  'layers',\n",
       "  'peel',\n",
       "  'off',\n",
       "  'and',\n",
       "  'syntaxes',\n",
       "  'consider',\n",
       "  'quite',\n",
       "  'challenge'],\n",
       " ['and', 'that', 'exactly', 'the', 'way', 'with', 'our', 'machines'],\n",
       " ['order',\n",
       "  'get',\n",
       "  'our',\n",
       "  'computer',\n",
       "  'understand',\n",
       "  'any',\n",
       "  'text',\n",
       "  'need',\n",
       "  'break',\n",
       "  'that',\n",
       "  'word',\n",
       "  'down',\n",
       "  'way',\n",
       "  'that',\n",
       "  'our',\n",
       "  'machine',\n",
       "  'can',\n",
       "  'understand'],\n",
       " ['that',\n",
       "  'where',\n",
       "  'the',\n",
       "  'concept',\n",
       "  'tokenization',\n",
       "  'natural',\n",
       "  'language',\n",
       "  'processing',\n",
       "  'nlp',\n",
       "  'comes'],\n",
       " ['simply',\n",
       "  'put',\n",
       "  'can',\n",
       "  'work',\n",
       "  'with',\n",
       "  'text',\n",
       "  'data',\n",
       "  'don',\n",
       "  'perform',\n",
       "  'tokenization'],\n",
       " ['yes', 'really', 'that', 'important']]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pre_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(pre_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'that': 1,\n",
       " 'tokenization': 2,\n",
       " 'language': 3,\n",
       " 'and': 4,\n",
       " 'with': 5,\n",
       " 'text': 6,\n",
       " 'the': 7,\n",
       " 'our': 8,\n",
       " 'data': 9,\n",
       " 'quite': 10,\n",
       " 'you': 11,\n",
       " 'way': 12,\n",
       " 'understand': 13,\n",
       " 'can': 14,\n",
       " 'key': 15,\n",
       " 'mandatory': 16,\n",
       " 'aspect': 17,\n",
       " 'working': 18,\n",
       " 'discuss': 19,\n",
       " 'various': 20,\n",
       " 'nuances': 21,\n",
       " 'including': 22,\n",
       " 'how': 23,\n",
       " 'handle': 24,\n",
       " 'out-of-vocabulary': 25,\n",
       " 'words': 26,\n",
       " 'oov': 27,\n",
       " 'thing': 28,\n",
       " 'beauty': 29,\n",
       " 'but': 30,\n",
       " 'mastering': 31,\n",
       " 'new': 32,\n",
       " 'from': 33,\n",
       " 'scratch': 34,\n",
       " 'daunting': 35,\n",
       " 'prospect': 36,\n",
       " 'ever': 37,\n",
       " 'picked': 38,\n",
       " 'wasn': 39,\n",
       " 'your': 40,\n",
       " 'mother': 41,\n",
       " 'tongue': 42,\n",
       " 'relate': 43,\n",
       " 'this': 44,\n",
       " 'there': 45,\n",
       " 'are': 46,\n",
       " 'many': 47,\n",
       " 'layers': 48,\n",
       " 'peel': 49,\n",
       " 'off': 50,\n",
       " 'syntaxes': 51,\n",
       " 'consider': 52,\n",
       " 'challenge': 53,\n",
       " 'exactly': 54,\n",
       " 'machines': 55,\n",
       " 'order': 56,\n",
       " 'get': 57,\n",
       " 'computer': 58,\n",
       " 'any': 59,\n",
       " 'need': 60,\n",
       " 'break': 61,\n",
       " 'word': 62,\n",
       " 'down': 63,\n",
       " 'machine': 64,\n",
       " 'where': 65,\n",
       " 'concept': 66,\n",
       " 'natural': 67,\n",
       " 'processing': 68,\n",
       " 'nlp': 69,\n",
       " 'comes': 70,\n",
       " 'simply': 71,\n",
       " 'put': 72,\n",
       " 'work': 73,\n",
       " 'don': 74,\n",
       " 'perform': 75,\n",
       " 'yes': 76,\n",
       " 'really': 77,\n",
       " 'important': 78}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('tokenization', 4),\n",
       "             ('key', 1),\n",
       "             ('and', 3),\n",
       "             ('mandatory', 1),\n",
       "             ('aspect', 1),\n",
       "             ('working', 1),\n",
       "             ('with', 3),\n",
       "             ('text', 3),\n",
       "             ('data', 2),\n",
       "             ('discuss', 1),\n",
       "             ('the', 3),\n",
       "             ('various', 1),\n",
       "             ('nuances', 1),\n",
       "             ('including', 1),\n",
       "             ('how', 1),\n",
       "             ('handle', 1),\n",
       "             ('out-of-vocabulary', 1),\n",
       "             ('words', 1),\n",
       "             ('oov', 1),\n",
       "             ('language', 4),\n",
       "             ('thing', 1),\n",
       "             ('beauty', 1),\n",
       "             ('but', 1),\n",
       "             ('mastering', 1),\n",
       "             ('new', 1),\n",
       "             ('from', 1),\n",
       "             ('scratch', 1),\n",
       "             ('quite', 2),\n",
       "             ('daunting', 1),\n",
       "             ('prospect', 1),\n",
       "             ('you', 2),\n",
       "             ('ever', 1),\n",
       "             ('picked', 1),\n",
       "             ('that', 6),\n",
       "             ('wasn', 1),\n",
       "             ('your', 1),\n",
       "             ('mother', 1),\n",
       "             ('tongue', 1),\n",
       "             ('relate', 1),\n",
       "             ('this', 1),\n",
       "             ('there', 1),\n",
       "             ('are', 1),\n",
       "             ('many', 1),\n",
       "             ('layers', 1),\n",
       "             ('peel', 1),\n",
       "             ('off', 1),\n",
       "             ('syntaxes', 1),\n",
       "             ('consider', 1),\n",
       "             ('challenge', 1),\n",
       "             ('exactly', 1),\n",
       "             ('way', 2),\n",
       "             ('our', 3),\n",
       "             ('machines', 1),\n",
       "             ('order', 1),\n",
       "             ('get', 1),\n",
       "             ('computer', 1),\n",
       "             ('understand', 2),\n",
       "             ('any', 1),\n",
       "             ('need', 1),\n",
       "             ('break', 1),\n",
       "             ('word', 1),\n",
       "             ('down', 1),\n",
       "             ('machine', 1),\n",
       "             ('can', 2),\n",
       "             ('where', 1),\n",
       "             ('concept', 1),\n",
       "             ('natural', 1),\n",
       "             ('processing', 1),\n",
       "             ('nlp', 1),\n",
       "             ('comes', 1),\n",
       "             ('simply', 1),\n",
       "             ('put', 1),\n",
       "             ('work', 1),\n",
       "             ('don', 1),\n",
       "             ('perform', 1),\n",
       "             ('yes', 1),\n",
       "             ('really', 1),\n",
       "             ('important', 1)])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok.word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**패딩** : 단어(PAD)에 대해 0으로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_sentences = [['driver', 'person'], ['driver', 'good', 'person'], ['driver', 'huge', 'person'], ['knew', 'bad'], ['bad', 'kept', 'huge', 'bad'], ['huge', 'bad']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 정수 인코딩\n",
    "tok = Tokenizer()\n",
    "tok.fit_on_texts(pre_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3], [2, 5, 3], [2, 4, 3], [6, 1], [1, 7, 4, 1], [4, 1]]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tok.texts_to_sequences(pre_sentences)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "maxlen = max(len(i) for i in encoded)\n",
    "maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3, 0, 0],\n",
       " [2, 5, 3, 0],\n",
       " [2, 4, 3, 0],\n",
       " [6, 1, 0, 0],\n",
       " [1, 7, 4, 1],\n",
       " [4, 1, 0, 0]]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for s in encoded:\n",
    "    while len(s) < maxlen:\n",
    "        s.append(0)\n",
    "\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 0, 0],\n",
       "       [2, 5, 3, 0],\n",
       "       [2, 4, 3, 0],\n",
       "       [6, 1, 0, 0],\n",
       "       [1, 7, 4, 1],\n",
       "       [4, 1, 0, 0]])"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.array(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "케라스를 이용한 패딩작업"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[2, 3], [2, 5, 3], [2, 4, 3], [6, 1], [1, 7, 4, 1], [4, 1]]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tok.texts_to_sequences(pre_sentences)\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [2, 5, 3, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [2, 4, 3, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [6, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 7, 4, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [4, 1, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paded = pad_sequences(encoded, padding='post', maxlen=10) # pre가 기본값\n",
    "paded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 3],\n",
       "       [2, 5],\n",
       "       [2, 4],\n",
       "       [6, 1],\n",
       "       [1, 7],\n",
       "       [4, 1]])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paded = pad_sequences(encoded, maxlen=2, truncating='post') # 뒤쪽이 잘림\n",
    "paded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**원핫 인코딩**   \n",
    "\n",
    "단어 종류 : 10개 -> 인코딩 => 머신러닝/딥러닝 언어모델링(분류/생성/이해...)   \n",
    "1~10번까지 번호 부여(1번: sky, 2번: computer, 3번: pencil, ...)   \n",
    "1 + 2 = 3 => sky + computer = pencil ???\n",
    "\n",
    "sky :      100   \n",
    "computer : 010   \n",
    "pencil :   001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "단어의 종류가 10만개   \n",
    "원핫 인코딩을 하게되면 각 단어는 10만차원 벡터공간에 임베딩   \n",
    "그럼 단어 하나에 40만 Byte...?      \n",
    "sky : 10000...000 (원핫 벡터) -> 차원 감소 -> [1.7 -3.5 0.3 1.2 7.4] 5차원 벡터공간 임베딩(밀집벡터)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['자연어', '처리', '공부', '를', '합니다']"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = okt.morphs(\"자연어처리 공부를 합니다\")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코퍼스(수집한 문서 전체)\n",
    "text = \"점심 메뉴로 소고기볶음밥 먹었습니다. 소고기볶음밥 너무 맛있어요. 또 먹을래요.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'소고기볶음밥': 1,\n",
       " '점심': 2,\n",
       " '메뉴로': 3,\n",
       " '먹었습니다': 4,\n",
       " '너무': 5,\n",
       " '맛있어요': 6,\n",
       " '또': 7,\n",
       " '먹을래요': 8}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tok = Tokenizer()\n",
    "tok.fit_on_texts([text])\n",
    "tok.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = \"내일 메뉴로 소고기볶음밥 또 나왔으면 좋겠다.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 1, 7]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded = tok.texts_to_sequences([test])[0]\n",
    "encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o_v = to_categorical(encoded)\n",
    "o_v"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py_3_10_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
