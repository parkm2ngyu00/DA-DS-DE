{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "avKNSGAYeyMV"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"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\" # 환경변수에 OPENAI_API_KEY를 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "1ytJZvlMe7oL"
      },
      "outputs": [],
      "source": [
        "import openai\n",
        "\n",
        "GPT_MODEL = \"gpt-3.5-turbo-0613\"\n",
        "openai.api_key = \"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\" # 환경변수에 OPENAI_API_KEY를 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "aEhCZiv-fPQb"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import requests\n",
        "from tenacity import retry, wait_random_exponential, stop_after_attempt\n",
        "from termcolor import colored"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "YwuOYa1jfXE7"
      },
      "outputs": [],
      "source": [
        "@retry(wait=wait_random_exponential(min=1, max=40), stop=stop_after_attempt(3))\n",
        "def chat_completion_request(messages, functions=None, function_call=None, model=GPT_MODEL):\n",
        "    headers = {\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"Authorization\": \"Bearer \" + openai.api_key,\n",
        "    }\n",
        "    json_data = {\"model\": model, \"messages\": messages}\n",
        "    if functions is not None:\n",
        "        json_data.update({\"functions\": functions})\n",
        "    if function_call is not None:\n",
        "        json_data.update({\"function_call\": function_call})\n",
        "    try:\n",
        "        response = requests.post(\n",
        "            \"https://api.openai.com/v1/chat/completions\",\n",
        "            headers=headers,\n",
        "            json=json_data,\n",
        "        )\n",
        "        return response\n",
        "    except Exception as e:\n",
        "        print(\"Unable to generate ChatCompletion response\")\n",
        "        print(f\"Exception: {e}\")\n",
        "        return e"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6Hzb7j9FfYbD"
      },
      "outputs": [],
      "source": [
        "def pretty_print_conversation(messages):\n",
        "    role_to_color = {\n",
        "        \"system\": \"red\",\n",
        "        \"user\": \"green\",\n",
        "        \"assistant\": \"blue\",\n",
        "        \"function\": \"magenta\",\n",
        "    }\n",
        "    formatted_messages = []\n",
        "    for message in messages:\n",
        "        if message[\"role\"] == \"system\":\n",
        "            formatted_messages.append(f\"system: {message['content']}\\n\")\n",
        "        elif message[\"role\"] == \"user\":\n",
        "            formatted_messages.append(f\"user: {message['content']}\\n\")\n",
        "        elif message[\"role\"] == \"assistant\" and message.get(\"function_call\"):\n",
        "            formatted_messages.append(f\"assistant: {message['function_call']}\\n\")\n",
        "        elif message[\"role\"] == \"assistant\" and not message.get(\"function_call\"):\n",
        "            formatted_messages.append(f\"assistant: {message['content']}\\n\")\n",
        "        elif message[\"role\"] == \"function\":\n",
        "            formatted_messages.append(f\"function ({message['name']}): {message['content']}\\n\")\n",
        "    for formatted_message in formatted_messages:\n",
        "        print(\n",
        "            colored(\n",
        "                formatted_message,\n",
        "                role_to_color[messages[formatted_messages.index(formatted_message)][\"role\"]],\n",
        "            )\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "eetw9yGDfpzc"
      },
      "outputs": [],
      "source": [
        "functions = [\n",
        "    {\n",
        "        \"name\": \"get_current_weather\",\n",
        "        \"description\": \"Get the current weather\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"location\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
        "                },\n",
        "                \"unit\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"enum\": [\"celsius\", \"fahrenheit\"],\n",
        "                    \"description\": \"The temperature unit to use. Infer this from the users location.\",\n",
        "                },\n",
        "            },\n",
        "            \"required\": [\"location\", \"unit\"],\n",
        "        },\n",
        "    }\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qLd83Mixf-zk"
      },
      "outputs": [],
      "source": [
        "messages = []\n",
        "\n",
        "messages.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
        "messages.append({\"role\": \"user\", \"content\": \"What's the weather like today\"})\n",
        "\n",
        "chat_response = chat_completion_request(\n",
        "    messages, functions=functions\n",
        ")\n",
        "\n",
        "assistant_message = chat_response.json()[\"choices\"][0][\"message\"]\n",
        "\n",
        "messages.append(assistant_message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3JWHuyItgVH7",
        "outputId": "7a5c1be2-3c72-49f4-af27-11547639a16e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'role': 'assistant',\n",
              " 'content': 'Sure, can you please provide me with your current location?'}"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "assistant_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "eKXQ4lPugZ_5"
      },
      "outputs": [],
      "source": [
        "messages.append({\"role\": \"user\", \"content\": \"seoul\"})\n",
        "\n",
        "chat_response = chat_completion_request(\n",
        "    messages, functions=functions\n",
        ")\n",
        "\n",
        "assistant_message = chat_response.json()[\"choices\"][0][\"message\"]\n",
        "\n",
        "messages.append(assistant_message)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "lhuJ_kVQidID"
      },
      "outputs": [],
      "source": [
        "response_message = assistant_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "swoGa4u4jLU0",
        "outputId": "8180393e-56b4-4311-e37c-ac5be85b67fa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'role': 'assistant',\n",
              " 'content': None,\n",
              " 'function_call': {'name': 'get_current_weather',\n",
              "  'arguments': '{\\n  \"location\": \"Seoul\",\\n  \"unit\": \"celsius\"\\n}'}}"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response_message"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ccZSkJxWjXpa"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "\n",
        "def get_current_weather(location, unit='섭씨'):\n",
        "    API_KEY = '58f1db949b3abc2ff173e19c47945eb4'\n",
        "    api = f\"https://api.openweathermap.org/data/2.5/weather?q={location}&appid={API_KEY}&lang=ko&units=metric\"\n",
        "    print(api)\n",
        "    result = requests.get(api)\n",
        "    print(result)\n",
        "    result = json.loads(result.text)\n",
        "    temperature = result['main']['temp']\n",
        "    return str(temperature)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xu0xYcDaieGa",
        "outputId": "283853cd-f27b-4492-96f3-4ec23bfeca38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "https://api.openweathermap.org/data/2.5/weather?q=Seoul&appid=58f1db949b3abc2ff173e19c47945eb4&lang=ko&units=metric\n",
            "<Response [200]>\n",
            "33.02\n",
            "{\n",
            "  \"id\": \"chatcmpl-7psvagdzrP6tLPgibXVwgWCSjXz3J\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1692600226,\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"The current temperature in Seoul is 33.02 degrees Celsius.\"\n",
            "      },\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 126,\n",
            "    \"completion_tokens\": 13,\n",
            "    \"total_tokens\": 139\n",
            "  }\n",
            "}\n",
            "The current temperature in Seoul is 33.02 degrees Celsius.\n"
          ]
        }
      ],
      "source": [
        " # Step 2: check if GPT wanted to call a function\n",
        "if response_message.get(\"function_call\"):\n",
        "    # Step 3: call the function\n",
        "    # Note: the JSON response may not always be valid; be sure to handle errors\n",
        "    available_functions = {\n",
        "        \"get_current_weather\": get_current_weather,\n",
        "    }  # only one function in this example, but you can have multiple\n",
        "    # 1. 함수 이름 획득\n",
        "    function_name = response_message[\"function_call\"][\"name\"]\n",
        "    # 2. 함수 이름으로 실제 함수 주소 획득\n",
        "    fuction_to_call = available_functions[function_name]\n",
        "\n",
        "    # 3. 인자 획득\n",
        "    function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
        "\n",
        "    # 4. 함수 호출 및 결과 획득 (함수 이름과 인자를 사용하여)\n",
        "    function_response = fuction_to_call(\n",
        "        location=function_args.get(\"location\"),\n",
        "        unit=function_args.get(\"unit\"),\n",
        "    )\n",
        "\n",
        "    print(function_response)\n",
        "\n",
        "    # Step 4: send the info on the function call and function response to GPT\n",
        "    # AI가 전달해준 함수 호출 정보\n",
        "    messages.append(response_message)  # extend conversation with assistant's reply\n",
        "    # 시스템이 함수 호출 후 결과 정보\n",
        "    messages.append(\n",
        "        {\n",
        "            \"role\": \"function\",\n",
        "            \"name\": function_name,\n",
        "            \"content\": function_response,\n",
        "        }\n",
        "    )  # extend conversation with function response\n",
        "\n",
        "    # 종합 메시지로 ai 메시지 요청\n",
        "    second_response = openai.ChatCompletion.create(\n",
        "        model=\"gpt-3.5-turbo-0613\",\n",
        "        messages=messages,\n",
        "    )  # get a new response from GPT where it can see the function response\n",
        "\n",
        "    # 사용자에게 줄 메시지 출력\n",
        "    print(second_response)\n",
        "    print(second_response[\"choices\"][0][\"message\"][\"content\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "prXZAUwV5Rfr"
      },
      "outputs": [],
      "source": [
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "i39FLfgU5SL6",
        "outputId": "8dcb13ed-9500-4558-fdd9-59f810ab5bc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Note: opening Chrome Inspector may crash demo inside Colab notebooks.\n",
            "\n",
            "To create a public link, set `share=True` in `launch()`.\n"
          ]
        },
        {
          "data": {
            "application/javascript": "(async (port, path, width, height, cache, element) => {\n                        if (!google.colab.kernel.accessAllowed && !cache) {\n                            return;\n                        }\n                        element.appendChild(document.createTextNode(''));\n                        const url = await google.colab.kernel.proxyPort(port, {cache});\n\n                        const external_link = document.createElement('div');\n                        external_link.innerHTML = `\n                            <div style=\"font-family: monospace; margin-bottom: 0.5rem\">\n                                Running on <a href=${new URL(path, url).toString()} target=\"_blank\">\n                                    https://localhost:${port}${path}\n                                </a>\n                            </div>\n                        `;\n                        element.appendChild(external_link);\n\n                        const iframe = document.createElement('iframe');\n                        iframe.src = new URL(path, url).toString();\n                        iframe.height = height;\n                        iframe.allow = \"autoplay; camera; microphone; clipboard-read; clipboard-write;\"\n                        iframe.width = width;\n                        iframe.style.border = 0;\n                        element.appendChild(iframe);\n                    })(7860, \"/\", \"100%\", 500, false, window.element)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'role': 'system', 'content': \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"}, {'role': 'user', 'content': 'seoul'}]\n",
            "<Response [200]>\n",
            "#chat_response#\n",
            "https://api.openweathermap.org/data/2.5/weather?q=Seoul&appid=58f1db949b3abc2ff173e19c47945eb4&lang=ko&units=metric\n",
            "<Response [200]>\n",
            "32.7\n",
            "{\n",
            "  \"id\": \"chatcmpl-7ptrE77ymmnaA9Q48cNuGFsGQWESt\",\n",
            "  \"object\": \"chat.completion\",\n",
            "  \"created\": 1692603800,\n",
            "  \"model\": \"gpt-3.5-turbo-0613\",\n",
            "  \"choices\": [\n",
            "    {\n",
            "      \"index\": 0,\n",
            "      \"message\": {\n",
            "        \"role\": \"assistant\",\n",
            "        \"content\": \"The current weather in Seoul is 32.7\\u00b0C. Is there anything else I can assist you with?\"\n",
            "      },\n",
            "      \"finish_reason\": \"stop\"\n",
            "    }\n",
            "  ],\n",
            "  \"usage\": {\n",
            "    \"prompt_tokens\": 72,\n",
            "    \"completion_tokens\": 22,\n",
            "    \"total_tokens\": 94\n",
            "  }\n",
            "}\n",
            "[{'role': 'system', 'content': \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"}, {'role': 'user', 'content': 'seoul'}, {'role': 'assistant', 'content': None, 'function_call': {'name': 'get_current_weather', 'arguments': '{\\n  \"location\": \"Seoul\",\\n  \"unit\": \"celsius\"\\n}'}}, {'role': 'function', 'name': 'get_current_weather', 'content': '32.7'}, <OpenAIObject at 0x7a60c5a5a570> JSON: {\n",
            "  \"role\": \"assistant\",\n",
            "  \"content\": \"The current weather in Seoul is 32.7\\u00b0C. Is there anything else I can assist you with?\"\n",
            "}, {'role': 'user', 'content': 'thanks'}]\n",
            "<Response [200]>\n",
            "#chat_response#\n",
            "Keyboard interruption in main thread... closing server.\n"
          ]
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 42,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def respond(user_message, chat_history, state_message_history):  # 채팅봇의 응답을 처리하는 함수를 정의합니다.\n",
        "\n",
        "    if len(state_message_history) == 0:\n",
        "        state_message_history.append({\"role\": \"system\", \"content\": \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"})\n",
        "\n",
        "    state_message_history.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    print(state_message_history)\n",
        "\n",
        "    chat_response = chat_completion_request(\n",
        "        state_message_history, functions=functions\n",
        "    )\n",
        "\n",
        "    print(chat_response)\n",
        "    print(\"#chat_response#\")\n",
        "\n",
        "    assistant_message = chat_response.json()[\"choices\"][0][\"message\"]\n",
        "\n",
        "    response_message = assistant_message\n",
        "\n",
        "\n",
        "     # Step 2: check if GPT wanted to call a function\n",
        "    if response_message.get(\"function_call\"):\n",
        "        # Step 3: call the function\n",
        "        # Note: the JSON response may not always be valid; be sure to handle errors\n",
        "        available_functions = {\n",
        "            \"get_current_weather\": get_current_weather,\n",
        "        }  # only one function in this example, but you can have multiple\n",
        "        # 1. 함수 이름 획득\n",
        "        function_name = response_message[\"function_call\"][\"name\"]\n",
        "        # 2. 함수 이름으로 실제 함수 주소 획득\n",
        "        fuction_to_call = available_functions[function_name]\n",
        "\n",
        "        # 3. 인자 획득\n",
        "        function_args = json.loads(response_message[\"function_call\"][\"arguments\"])\n",
        "\n",
        "        # 4. 함수 호출 및 결과 획득 (함수 이름과 인자를 사용하여)\n",
        "        function_response = fuction_to_call(\n",
        "            location=function_args.get(\"location\"),\n",
        "            unit=function_args.get(\"unit\"),\n",
        "        )\n",
        "\n",
        "        print(function_response)\n",
        "\n",
        "        # Step 4: send the info on the function call and function response to GPT\n",
        "        # AI가 전달해준 함수 호출 정보\n",
        "        state_message_history.append(response_message)  # extend conversation with assistant's reply\n",
        "        # 시스템이 함수 호출 후 결과 정보\n",
        "        state_message_history.append(\n",
        "            {\n",
        "                \"role\": \"function\",\n",
        "                \"name\": function_name,\n",
        "                \"content\": function_response,\n",
        "            }\n",
        "        )  # extend conversation with function response\n",
        "\n",
        "        # 종합 메시지로 ai 메시지 요청\n",
        "        second_response = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo-0613\",\n",
        "            messages=state_message_history,\n",
        "        )  # get a new response from GPT where it can see the function response\n",
        "\n",
        "        state_message_history.append(second_response[\"choices\"][0][\"message\"])\n",
        "        print(second_response)\n",
        "        bot_message = second_response[\"choices\"][0][\"message\"]['content']\n",
        "    else:\n",
        "        state_message_history.append(assistant_message)\n",
        "        bot_message = assistant_message['content']\n",
        "\n",
        "    chat_history.append((user_message, bot_message))  # 채팅 기록에 사용자의 메시지와 봇의 응답을 추가합니다.\n",
        "\n",
        "    return \"\", chat_history, state_message_history  # 수정된 채팅 기록을 반환합니다.\n",
        "\n",
        "with gr.Blocks() as demo:  # gr.Blocks()를 사용하여 인터페이스를 생성합니다.\n",
        "\n",
        "    state_message_history = gr.State([])\n",
        "\n",
        "    chatbot = gr.Chatbot(label=\"채팅창\")  # '채팅창'이라는 레이블을 가진 채팅봇 컴포넌트를 생성합니다.\n",
        "    msg = gr.Textbox(label=\"입력\")  # '입력'이라는 레이블을 가진 텍스트박스를 생성합니다.\n",
        "    clear = gr.Button(\"초기화\")  # '초기화'라는 레이블을 가진 버튼을 생성합니다.\n",
        "\n",
        "    msg.submit(respond, [msg, chatbot, state_message_history], [msg, chatbot, state_message_history])  # 텍스트박스에 메시지를 입력하고 제출하면 respond 함수가 호출되도록 합니다.\n",
        "    clear.click(lambda: None, None, chatbot, queue=False)  # '초기화' 버튼을 클릭하면 채팅 기록을 초기화합니다.\n",
        "\n",
        "demo.launch(debug=True)  # 인터페이스를 실행합니다. 실행하면 사용자는 '입력' 텍스트박스에 메시지를 작성하고 제출할 수 있으며, '초기화' 버튼을 통해 채팅 기록을 초기화 할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RfqWpstd7H2K"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
