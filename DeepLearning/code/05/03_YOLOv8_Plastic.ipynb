{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t6MPjfT5NrKQ"
   },
   "source": [
    "# YOLOv8: CoCo128 튜토리얼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7mGmQbAO5pQb"
   },
   "source": [
    "## 1. 환경설정 Setup\n",
    "\n",
    "- https://github.com/ultralytics/ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wbvMlHd_QwMG",
    "outputId": "27ca383c-0a97-4679-f1c5-ba843f033de7",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.0.152  Python-3.10.4 torch-2.0.1+cpu CPU (Intel Core(TM) i5-8265U 1.60GHz)\n",
      "Setup complete  (8 CPUs, 7.8 GB RAM, 158.1/221.8 GB disk)\n"
     ]
    }
   ],
   "source": [
    "# %pip install ultralytics\n",
    "\n",
    "import ultralytics\n",
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZY2VXXXu74w5",
    "tags": []
   },
   "source": [
    "## 2. Train\n",
    "\n",
    "<p align=\"\"><a href=\"https://bit.ly/ultralytics_hub\"><img width=\"1000\" src=\"https://github.com/ultralytics/assets/raw/main/yolov8/banner-integrations.png\"/></a></p>\n",
    "\n",
    "Train YOLOv8 on [Detect](https://docs.ultralytics.com/tasks/detect/), [Segment](https://docs.ultralytics.com/tasks/segment/), [Classify](https://docs.ultralytics.com/tasks/classify/) and [Pose](https://docs.ultralytics.com/tasks/pose/) datasets. See [YOLOv8 Train Docs](https://docs.ultralytics.com/modes/train/) for more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    897664  ultralytics.nn.modules.head.Detect           [80, [64, 128, 256]]          \n",
      "YOLOv8n summary: 225 layers, 3157200 parameters, 3157184 gradients\n",
      "\n",
      "Ultralytics YOLOv8.0.152  Python-3.10.4 torch-2.0.1+cpu CPU (Intel Core(TM) i5-8265U 1.60GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=plastic.yaml, epochs=100, patience=50, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=None, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, show=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, vid_stride=1, line_width=None, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, boxes=True, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train5\n",
      "Overriding model.yaml nc=80 with nc=4\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
      "Model summary: 225 layers, 3011628 parameters, 3011612 gradients\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "WARNING  TensorBoard not initialized correctly, not logging this run. runs\\detect\\train5 is not a directory\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\parkm\\OneDrive\\바탕 화면\\Bigleader\\DeepLearning\\code\\dataset\\plastic-yolo\\train\\labels.cache... 17 images, 0 backgrounds, 0 corrupt: 100%|██████████| 17/17 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\parkm\\OneDrive\\바탕 화면\\Bigleader\\DeepLearning\\code\\dataset\\plastic-yolo\\val\\labels.cache... 8 images, 0 backgrounds, 0 corrupt: 100%|██████████| 8/8 [00:00<?, ?it/s]\n",
      "Plotting labels to runs\\detect\\train5\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train5\u001b[0m\n",
      "Starting training for 100 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "Image Not Found C:\\Users\\parkm\\OneDrive\\바탕 화면\\Bigleader\\DeepLearning\\code\\dataset\\plastic-yolo\\train\\images\\plastic_test_008.jpg",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\parkm\\OneDrive\\바탕 화면\\Bigleader\\DeepLearning\\code\\05\\03_YOLOv8_Plastic.ipynb Cell 5\u001b[0m in \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/parkm/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Bigleader/DeepLearning/code/05/03_YOLOv8_Plastic.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model \u001b[39m=\u001b[39m YOLO(\u001b[39m'\u001b[39m\u001b[39myolov8n.pt\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# load a pretrained model (recommended for training)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/parkm/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Bigleader/DeepLearning/code/05/03_YOLOv8_Plastic.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Use the model\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/parkm/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Bigleader/DeepLearning/code/05/03_YOLOv8_Plastic.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mtrain(data\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mplastic.yaml\u001b[39;49m\u001b[39m'\u001b[39;49m, epochs\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m)  \u001b[39m# train the model\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/parkm/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Bigleader/DeepLearning/code/05/03_YOLOv8_Plastic.ipynb#W4sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m results \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mval()  \u001b[39m# evaluate model performance on the validation set\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/parkm/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Bigleader/DeepLearning/code/05/03_YOLOv8_Plastic.ipynb#W4sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m#results = model('https://ultralytics.com/images/bus.jpg')  # predict on an image\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/parkm/OneDrive/%EB%B0%94%ED%83%95%20%ED%99%94%EB%A9%B4/Bigleader/DeepLearning/code/05/03_YOLOv8_Plastic.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m#results = model.export(format='onnx')  # export the model to ONNX format\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\parkm\\anaconda3\\envs\\py_3_10_4\\lib\\site-packages\\ultralytics\\engine\\model.py:377\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, trainer, **kwargs)\u001b[0m\n\u001b[0;32m    375\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mmodel\n\u001b[0;32m    376\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mhub_session \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession  \u001b[39m# attach optional HUB session\u001b[39;00m\n\u001b[1;32m--> 377\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49mtrain()\n\u001b[0;32m    378\u001b[0m \u001b[39m# Update model and cfg after training\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[39mif\u001b[39;00m RANK \u001b[39min\u001b[39;00m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\parkm\\anaconda3\\envs\\py_3_10_4\\lib\\site-packages\\ultralytics\\engine\\trainer.py:192\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    190\u001b[0m         ddp_cleanup(\u001b[39mself\u001b[39m, \u001b[39mstr\u001b[39m(file))\n\u001b[0;32m    191\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 192\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_train(world_size)\n",
      "File \u001b[1;32mc:\\Users\\parkm\\anaconda3\\envs\\py_3_10_4\\lib\\site-packages\\ultralytics\\engine\\trainer.py:315\u001b[0m, in \u001b[0;36mBaseTrainer._do_train\u001b[1;34m(self, world_size)\u001b[0m\n\u001b[0;32m    313\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtloss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    314\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> 315\u001b[0m \u001b[39mfor\u001b[39;00m i, batch \u001b[39min\u001b[39;00m pbar:\n\u001b[0;32m    316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_callbacks(\u001b[39m'\u001b[39m\u001b[39mon_train_batch_start\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    317\u001b[0m     \u001b[39m# Warmup\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\parkm\\anaconda3\\envs\\py_3_10_4\\lib\\site-packages\\tqdm\\std.py:1178\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1175\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[0;32m   1177\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m-> 1178\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m   1179\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[0;32m   1180\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[0;32m   1181\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\parkm\\anaconda3\\envs\\py_3_10_4\\lib\\site-packages\\ultralytics\\data\\build.py:38\u001b[0m, in \u001b[0;36mInfiniteDataLoader.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[39m\"\"\"Creates a sampler that repeats indefinitely.\"\"\"\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m)):\n\u001b[1;32m---> 38\u001b[0m     \u001b[39myield\u001b[39;00m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterator)\n",
      "File \u001b[1;32mc:\\Users\\parkm\\anaconda3\\envs\\py_3_10_4\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\parkm\\anaconda3\\envs\\py_3_10_4\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\parkm\\anaconda3\\envs\\py_3_10_4\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\parkm\\anaconda3\\envs\\py_3_10_4\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\parkm\\anaconda3\\envs\\py_3_10_4\\lib\\site-packages\\ultralytics\\data\\base.py:240\u001b[0m, in \u001b[0;36mBaseDataset.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    238\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, index):\n\u001b[0;32m    239\u001b[0m     \u001b[39m\"\"\"Returns transformed label information for given index.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 240\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_image_and_label(index))\n",
      "File \u001b[1;32mc:\\Users\\parkm\\anaconda3\\envs\\py_3_10_4\\lib\\site-packages\\ultralytics\\data\\base.py:246\u001b[0m, in \u001b[0;36mBaseDataset.get_image_and_label\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    244\u001b[0m label \u001b[39m=\u001b[39m deepcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[index])  \u001b[39m# requires deepcopy() https://github.com/ultralytics/ultralytics/pull/1948\u001b[39;00m\n\u001b[0;32m    245\u001b[0m label\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)  \u001b[39m# shape is for rect, remove it\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m label[\u001b[39m'\u001b[39m\u001b[39mimg\u001b[39m\u001b[39m'\u001b[39m], label[\u001b[39m'\u001b[39m\u001b[39mori_shape\u001b[39m\u001b[39m'\u001b[39m], label[\u001b[39m'\u001b[39m\u001b[39mresized_shape\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_image(index)\n\u001b[0;32m    247\u001b[0m label[\u001b[39m'\u001b[39m\u001b[39mratio_pad\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m (label[\u001b[39m'\u001b[39m\u001b[39mresized_shape\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m label[\u001b[39m'\u001b[39m\u001b[39mori_shape\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m],\n\u001b[0;32m    248\u001b[0m                       label[\u001b[39m'\u001b[39m\u001b[39mresized_shape\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m1\u001b[39m] \u001b[39m/\u001b[39m label[\u001b[39m'\u001b[39m\u001b[39mori_shape\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m1\u001b[39m])  \u001b[39m# for evaluation\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrect:\n",
      "File \u001b[1;32mc:\\Users\\parkm\\anaconda3\\envs\\py_3_10_4\\lib\\site-packages\\ultralytics\\data\\base.py:153\u001b[0m, in \u001b[0;36mBaseDataset.load_image\u001b[1;34m(self, i)\u001b[0m\n\u001b[0;32m    151\u001b[0m     im \u001b[39m=\u001b[39m cv2\u001b[39m.\u001b[39mimread(f)  \u001b[39m# BGR\u001b[39;00m\n\u001b[0;32m    152\u001b[0m     \u001b[39mif\u001b[39;00m im \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 153\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mImage Not Found \u001b[39m\u001b[39m{\u001b[39;00mf\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    154\u001b[0m h0, w0 \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mshape[:\u001b[39m2\u001b[39m]  \u001b[39m# orig hw\u001b[39;00m\n\u001b[0;32m    155\u001b[0m r \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimgsz \u001b[39m/\u001b[39m \u001b[39mmax\u001b[39m(h0, w0)  \u001b[39m# ratio\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Image Not Found C:\\Users\\parkm\\OneDrive\\바탕 화면\\Bigleader\\DeepLearning\\code\\dataset\\plastic-yolo\\train\\images\\plastic_test_008.jpg"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('yolov8n.yaml')  # build a new model from scratch\n",
    "model = YOLO('yolov8n.pt')  # load a pretrained model (recommended for training)\n",
    "\n",
    "# Use the model\n",
    "results = model.train(data='plastic.yaml', epochs=100)  # train the model\n",
    "results = model.val()  # evaluate model performance on the validation set\n",
    "\n",
    "#results = model('https://ultralytics.com/images/bus.jpg')  # predict on an image\n",
    "#results = model.export(format='onnx')  # export the model to ONNX format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Prediction\n",
    "- prediction 저장: `model(source, save=True`)\n",
    "- prediction 스트리밍:`model(source, stream=True)`\n",
    "- prediction 장치 소스:`model(source=0, show=True)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Youtube Video Inference\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pretrained YOLOv8n model\n",
    "model = YOLO('runs/detect/train/weights/best.pt')\n",
    "\n",
    "# Define source as YouTube video URL\n",
    "source = 'https://youtu.be/QbKA9uNgzYQ'\n",
    "\n",
    "# Run inference on the source\n",
    "results = model(source, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Video Camera Inference\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('runs/detect/train/weights/best.pt')\n",
    "model.predict(source=\"0\", show=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Phm9ccmOKye5"
   },
   "source": [
    "## 4. Tasks\n",
    "\n",
    "YOLOv8 can train, val, predict and export models for the most common tasks in vision AI: [Detect](https://docs.ultralytics.com/tasks/detect/), [Segment](https://docs.ultralytics.com/tasks/segment/), [Classify](https://docs.ultralytics.com/tasks/classify/) and [Pose](https://docs.ultralytics.com/tasks/pose/). See [YOLOv8 Tasks Docs](https://docs.ultralytics.com/tasks/) for more information.\n",
    "\n",
    "<br><img width=\"1024\" src=\"https://raw.githubusercontent.com/ultralytics/assets/main/im/banner-tasks.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yq26lwpYK1lq"
   },
   "source": [
    "### 3.1 Detection\n",
    "\n",
    "YOLOv8 _detection_ models have no suffix and are the default YOLOv8 models, i.e. `yolov8n.pt` and are pretrained on COCO. See [Detection Docs](https://docs.ultralytics.com/tasks/detect/) for full details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Go5qqS9LbC5"
   },
   "outputs": [],
   "source": [
    "# Load YOLOv8n, train it on COCO128 for 3 epochs and predict an image with it\n",
    "from ultralytics import YOLO\n",
    "\n",
    "#model = YOLO('yolov8n.pt')  # load a pretrained YOLOv8n detection model\n",
    "#model.train(data='coco128.yaml', epochs=3)  # train the model\n",
    "#model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ZW58jUzK66B"
   },
   "source": [
    "### 3.2 Segmentation\n",
    "\n",
    "YOLOv8 _segmentation_ models use the `-seg` suffix, i.e. `yolov8n-seg.pt` and are pretrained on COCO. See [Segmentation Docs](https://docs.ultralytics.com/tasks/segment/) for full details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFPJIQl_L5HT"
   },
   "outputs": [],
   "source": [
    "# Load YOLOv8n-seg, train it on COCO128-seg for 3 epochs and predict an image with it\n",
    "from ultralytics import YOLO\n",
    "\n",
    "#model = YOLO('yolov8n-seg.pt')  # load a pretrained YOLOv8n segmentation model\n",
    "#model.train(data='coco128-seg.yaml', epochs=3)  # train the model\n",
    "model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ax3p94VNK9zR"
   },
   "source": [
    "### 3.3 Classification\n",
    "\n",
    "YOLOv8 _classification_ models use the `-cls` suffix, i.e. `yolov8n-cls.pt` and are pretrained on ImageNet. See [Classification Docs](https://docs.ultralytics.com/tasks/classify/) for full details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5q9Zu6zlL5rS"
   },
   "outputs": [],
   "source": [
    "# Load YOLOv8n-cls, train it on mnist160 for 3 epochs and predict an image with it\n",
    "from ultralytics import YOLO\n",
    "\n",
    "#model = YOLO('yolov8n-cls.pt')  # load a pretrained YOLOv8n classification model\n",
    "#model.train(data='mnist160', epochs=3)  # train the model\n",
    "#model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SpIaFLiO11TG"
   },
   "source": [
    "### 3.4. Pose\n",
    "\n",
    "YOLOv8 _pose_ models use the `-pose` suffix, i.e. `yolov8n-pose.pt` and are pretrained on COCO Keypoints. See [Pose Docs](https://docs.ultralytics.com/tasks/pose/) for full details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "si4aKFNg19vX"
   },
   "outputs": [],
   "source": [
    "# Load YOLOv8n-pose, train it on COCO8-pose for 3 epochs and predict an image with it\n",
    "from ultralytics import YOLO\n",
    "\n",
    "#model = YOLO('yolov8n-pose.pt')  # load a pretrained YOLOv8n classification model\n",
    "#model.train(data='coco8-pose.yaml', epochs=3)  # train the model\n",
    "#model('https://ultralytics.com/images/bus.jpg')  # predict on an image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPZZeNrLCQG6",
    "tags": []
   },
   "source": [
    "## 4. Export\n",
    "\n",
    "Export a YOLOv8 model to any supported format below with the `format` argument, i.e. `format=onnx`. See [YOLOv8 Export Docs](https://docs.ultralytics.com/modes/export/) for more information.\n",
    "\n",
    "- 💡 ProTip: Export to [ONNX](https://onnx.ai/) or [OpenVINO](https://docs.openvino.ai/latest/index.html) for up to 3x CPU speedup.  \n",
    "- 💡 ProTip: Export to [TensorRT](https://developer.nvidia.com/tensorrt) for up to 5x GPU speedup.\n",
    "\n",
    "\n",
    "| Format                                                             | `format` Argument | Model                     | Metadata | Arguments                                           |\n",
    "|--------------------------------------------------------------------|-------------------|---------------------------|----------|-----------------------------------------------------|\n",
    "| [PyTorch](https://pytorch.org/)                                    | -                 | `yolov8n.pt`              | ✅        | -                                                   |\n",
    "| [TorchScript](https://pytorch.org/docs/stable/jit.html)            | `torchscript`     | `yolov8n.torchscript`     | ✅        | `imgsz`, `optimize`                                 |\n",
    "| [ONNX](https://onnx.ai/)                                           | `onnx`            | `yolov8n.onnx`            | ✅        | `imgsz`, `half`, `dynamic`, `simplify`, `opset`     |\n",
    "| [OpenVINO](https://docs.openvino.ai/latest/index.html)             | `openvino`        | `yolov8n_openvino_model/` | ✅        | `imgsz`, `half`                                     |\n",
    "| [TensorRT](https://developer.nvidia.com/tensorrt)                  | `engine`          | `yolov8n.engine`          | ✅        | `imgsz`, `half`, `dynamic`, `simplify`, `workspace` |\n",
    "| [CoreML](https://github.com/apple/coremltools)                     | `coreml`          | `yolov8n.mlmodel`         | ✅        | `imgsz`, `half`, `int8`, `nms`                      |\n",
    "| [TF SavedModel](https://www.tensorflow.org/guide/saved_model)      | `saved_model`     | `yolov8n_saved_model/`    | ✅        | `imgsz`, `keras`                                    |\n",
    "| [TF GraphDef](https://www.tensorflow.org/api_docs/python/tf/Graph) | `pb`              | `yolov8n.pb`              | ❌        | `imgsz`                                             |\n",
    "| [TF Lite](https://www.tensorflow.org/lite)                         | `tflite`          | `yolov8n.tflite`          | ✅        | `imgsz`, `half`, `int8`                             |\n",
    "| [TF Edge TPU](https://coral.ai/docs/edgetpu/models-intro/)         | `edgetpu`         | `yolov8n_edgetpu.tflite`  | ✅        | `imgsz`                                             |\n",
    "| [TF.js](https://www.tensorflow.org/js)                             | `tfjs`            | `yolov8n_web_model/`      | ✅        | `imgsz`                                             |\n",
    "| [PaddlePaddle](https://github.com/PaddlePaddle)                    | `paddle`          | `yolov8n_paddle_model/`   | ✅        | `imgsz`                                             |\n",
    "| [ncnn](https://github.com/Tencent/ncnn)                            | `ncnn`            | `yolov8n_ncnn_model/`     | ✅        | `imgsz`, `half`                                     |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = model.export(format='onnx')  # export the model to ONNX format"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "YOLOv8 Tutorial",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "py_3_10_4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
